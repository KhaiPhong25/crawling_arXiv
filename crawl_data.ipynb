{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a0e80a8",
   "metadata": {},
   "source": [
    "# <center> **Milestone 1 ‚Äî Module 1: The Downloader (arXiv TeX)**\n",
    "**Student (Representative):** Vu Tuan Hung \n",
    "\n",
    "**Student ID:** 22127137  \n",
    "\n",
    "**Range:** 2022-11-13747 ‚Üí 2022-12-11475\n",
    "\n",
    "\n",
    "**Optimizations:**\n",
    "- Parallel download with ThreadPoolExecutor (tuned for MacBook Pro M4 Pro, 24GB RAM).\n",
    "- Resume-safe: skip parts already complete (size matches S3); re-download only mismatched/zero-byte files.\n",
    "- Extract-once with `.done` marker.\n",
    "- Skip processing for papers already produced (`metadata.json` & `references.json`).\n",
    "- Live counters: parts left per month and in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4245320",
   "metadata": {},
   "source": [
    "## **Download and Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "b914fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 1. Install dependencies\n",
    "!pip -q install boto3 arxiv requests tqdm python-dateutil sickle filetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "f2f6aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imports & constants\n",
    "from __future__ import annotations\n",
    "import os, re, tarfile, shutil, json, time, subprocess, threading\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Tuple, Iterable\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser as dtparser\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "try:\n",
    "    import boto3  # kh√¥ng b·∫Øt bu·ªôc\n",
    "    from botocore.config import Config as BotoConfig\n",
    "except Exception:\n",
    "    boto3 = None\n",
    "\n",
    "import requests, arxiv, gzip, filetype, io, struct, hashlib, unicodedata\n",
    "import xml.etree.ElementTree as ET\n",
    "from sickle import Sickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adaf07",
   "metadata": {},
   "source": [
    "## **Config and constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "9d7b5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARXIV_S3_BUCKET = \"arxiv\"\n",
    "ARXIV_S3_PREFIX = \"src\"   # s3://arxiv/src/arXiv_src_YYMM_XXX.tar\n",
    "PRINT_LOCK = threading.Lock()\n",
    "\n",
    "# 3. Config\n",
    "START_MONTH, START_ID = \"2022-11\", 13747\n",
    "#END_MONTH,   END_ID   = \"2022-12\", 11475\n",
    "END_MONTH,   END_ID   = \"2022-11\", 13755\n",
    "\n",
    "# Khuy·∫øn ngh·ªã theo m·∫°ng c·ªßa b·∫°n (Downlink ~350 Mb/s):\n",
    "MAX_WORKERS = 4       # th·ª≠ 12; n·∫øu dao ƒë·ªông/l·ªói v·∫∑t, h·∫° 10\n",
    "CHUNK_MBPS_EST = 11.0  # MB/s ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng ETA (kh√¥ng ·∫£nh h∆∞·ªüng logic)\n",
    "\n",
    "OUT_DIR  = Path(\"output_data\")\n",
    "WORK_DIR = Path(\"workdir\")\n",
    "MAX_REFS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d825911",
   "metadata": {},
   "source": [
    "## **Crawl metadata**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f46cc",
   "metadata": {},
   "source": [
    "### **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "4141236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "4d56b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_to_yymm(month_str: str):\n",
    "    return dtparser.parse(month_str + \"-01\").strftime(\"%y%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "b485c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_id_to_tuple(base_id: str):\n",
    "    m = re.fullmatch(r\"(\\d{4})\\.(\\d{4,5})\", base_id)\n",
    "    if not m: raise ValueError(f\"Bad arXiv base id format: {base_id}\")\n",
    "    return int(m.group(1)), int(m.group(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "faae94be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_range(yyyymm: int, iid: int, start_yyyymm: int, start_id: int, end_yyyymm: int, end_id: int):\n",
    "    if yyyymm < start_yyyymm or yyyymm > end_yyyymm: return False\n",
    "    if yyyymm == start_yyyymm and iid < start_id:     return False\n",
    "    if yyyymm == end_yyyymm   and iid > end_id:       return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "98b83c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _atomic_write_json(path: Path, data: Any, indent: int = 2):\n",
    "    \"\"\"Ghi JSON an to√†n: ghi v√†o file t·∫°m r·ªìi replace -> tr√°nh file b·ªã h·ªèng n·∫øu ƒëang ghi m√† crash.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=indent)\n",
    "    tmp.replace(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "edd28a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state(path: Path, state: Dict[str, Any]):\n",
    "    \"\"\"L∆∞u state (ghi an to√†n).\"\"\"\n",
    "    _atomic_write_json(path, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "38520d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas_dir = WORK_DIR / \"metadata\"\n",
    "papers_dir  = WORK_DIR / \"paper\"\n",
    "extracted = WORK_DIR / \"extracted\"\n",
    "\n",
    "ensure_dir(metadatas_dir)\n",
    "ensure_dir(papers_dir)\n",
    "ensure_dir(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05868cb",
   "metadata": {},
   "source": [
    "## **Crawling metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "92bc7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metadata_worker(result: arxiv.Result,\n",
    "                          client: arxiv.Client,\n",
    "                          checkpoint_folder: Path,\n",
    "                          query_delay: float = 3.0):\n",
    "    \"\"\"\n",
    "    Worker ƒë·ªÉ t·∫£i metadata c·ªßa m·ªôt b√†i b√°o duy nh·∫•t.\n",
    "    Ghi metadata tr·ª±c ti·∫øp v√†o th∆∞ m·ª•c checkpoint_folder (kh√¥ng t·∫°o th∆∞ m·ª•c l·ªìng).\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # L·∫•y arXiv ID v√† s·ªë phi√™n b·∫£n\n",
    "        arxiv_id = result.get_short_id()\n",
    "        v_pos = arxiv_id.rfind('v')\n",
    "        version_counts = int(arxiv_id[v_pos + 1:])\n",
    "        base_id = arxiv_id[:v_pos]  # ID g·ªëc, kh√¥ng g·ªìm version\n",
    "\n",
    "        # ƒê∆∞·ªùng d·∫´n metadata.json trong checkpoint_folder hi·ªán t·∫°i\n",
    "        meta_path = checkpoint_folder / \"metadata.json\"\n",
    "\n",
    "        # N·∫øu file ƒë√£ t·ªìn t·∫°i ‚Üí b·ªè qua\n",
    "        if meta_path.exists():\n",
    "            print(f\"Metadata already exists for {base_id}\")\n",
    "            return 1\n",
    "\n",
    "        print(f\"Creating metadata for {base_id}...\")\n",
    "\n",
    "        # Kh·ªüi t·∫°o dictionary metadata c∆° b·∫£n\n",
    "        metadata = {\n",
    "            \"id\": base_id,\n",
    "            \"title\": result.title,\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"abstract\": result.summary,\n",
    "            \"publication_venue\": [],\n",
    "            \"categories\": result.categories,\n",
    "            \"submission_date\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "            \"revised_dates\": [],\n",
    "            \"dois\": [],\n",
    "            \"comments\": [],\n",
    "            \"pdf_urls\": [],\n",
    "        }\n",
    "\n",
    "        # L·∫•y th√¥ng tin t·∫•t c·∫£ c√°c version c·ªßa paper\n",
    "        sub_search = arxiv.Search(\n",
    "            id_list=[f\"{base_id}v{ver}\" for ver in range(1, version_counts + 1)]\n",
    "        )\n",
    "        time.sleep(query_delay)\n",
    "\n",
    "        for sub_result in client.results(sub_search):\n",
    "            # C·∫≠p nh·∫≠t metadata t·ª´ t·ª´ng version\n",
    "            metadata[\"authors\"] = list(set(metadata[\"authors\"] + [a.name for a in sub_result.authors]))\n",
    "            if sub_result.journal_ref:\n",
    "                metadata[\"publication_venue\"].append(sub_result.journal_ref)\n",
    "            metadata[\"revised_dates\"].append(sub_result.updated.strftime(\"%Y-%m-%d\"))\n",
    "            if sub_result.doi:\n",
    "                metadata[\"dois\"].append(sub_result.doi)\n",
    "            if sub_result.comment:\n",
    "                metadata[\"comments\"].append(sub_result.comment)\n",
    "            metadata[\"pdf_urls\"].append(sub_result.pdf_url)\n",
    "\n",
    "        # Lo·∫°i b·ªè tr√πng l·∫∑p\n",
    "        metadata[\"publication_venue\"] = list(set(metadata[\"publication_venue\"]))\n",
    "        metadata[\"dois\"] = list(set(metadata[\"dois\"]))\n",
    "\n",
    "        # Ghi tr·ª±c ti·∫øp v√†o checkpoint_folder, KH√îNG t·∫°o th√™m th∆∞ m·ª•c ID\n",
    "        save_state(meta_path, metadata)\n",
    "        print(f\"Fetched metadata for arXiv ID {base_id}\")\n",
    "        return 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to fetch metadata for {result.get_short_id()}: {e}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "6e52af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_state(checkpoint_folder: Optional[Path], end_yymm: int, end_id: int):\n",
    "    \"\"\"\n",
    "    Ki·ªÉm tra v√† load checkpoint g·∫ßn nh·∫•t, tr·∫£ v·ªÅ:\n",
    "    (start_yymm, id_num, checkpoint_id, total_time, last_yymm)\n",
    "    \"\"\"\n",
    "    id_num = 1\n",
    "    checkpoint_id = 1\n",
    "    total_time = 0\n",
    "    start_yymm = None\n",
    "\n",
    "    if not checkpoint_folder or not checkpoint_folder.exists():\n",
    "        return None, id_num, checkpoint_id, total_time, None\n",
    "\n",
    "    checkpoints = list(checkpoint_folder.glob(\"metadata_checkpoint_*.json\"))\n",
    "    if not checkpoints:\n",
    "        return None, id_num, checkpoint_id, total_time, None\n",
    "\n",
    "    latest = max(checkpoints, key=lambda p: int(p.stem.split('_')[-1].split('.')[0]))\n",
    "    print(f\"Resuming from checkpoint: {latest}\")\n",
    "    checkpoint_id = int(latest.stem.split('_')[-1]) + 1\n",
    "\n",
    "    with latest.open(\"r\") as f:\n",
    "        state = json.load(f)\n",
    "        last_crawled_id = state['last_id']\n",
    "        total_time = state['total_time']\n",
    "        last_yymm, last_id_num = base_id_to_tuple(last_crawled_id)\n",
    "\n",
    "        if last_yymm == end_yymm and last_id_num >= end_id:\n",
    "            print(\"Crawling already completed up to the end ID.\")\n",
    "            return None, None, None, None, None\n",
    "\n",
    "        start_yymm, id_num = last_yymm, last_id_num + 1\n",
    "\n",
    "    return start_yymm, id_num, checkpoint_id, total_time, last_yymm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "12bde9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_id_list(start_yymm: int, start_id: int, end_yymm: int, end_id: int):\n",
    "    \"\"\"\n",
    "    T·∫°o danh s√°ch ID arXiv c·∫ßn crawl trong kho·∫£ng t·ª´ start ƒë·∫øn end.\n",
    "    \"\"\"\n",
    "    id_list = []\n",
    "    yymm = start_yymm\n",
    "\n",
    "    if start_yymm == end_yymm:\n",
    "        id_list = [f\"{yymm}.{i:05d}\" for i in range(start_id, end_id + 1)]\n",
    "    else:\n",
    "        while yymm <= end_yymm:\n",
    "            if yymm == start_yymm:\n",
    "                id_list.extend([f\"{yymm}.{i:05d}\" for i in range(start_id, 30001)])\n",
    "            elif yymm == end_yymm:\n",
    "                id_list.extend([f\"{yymm}.{i:05d}\" for i in range(1, end_id + 1)])\n",
    "            else:\n",
    "                id_list.extend([f\"{yymm}.{i:05d}\" for i in range(1, 30001)])\n",
    "            if yymm % 100 == 12:\n",
    "                yymm += 88  # sang nƒÉm m·ªõi\n",
    "            yymm += 1\n",
    "            start_id = 1\n",
    "    return id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "c7536aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_folder: Path,\n",
    "                    checkpoint_id: int,\n",
    "                    last_id: str,\n",
    "                    section_time: float,\n",
    "                    total_time: float):\n",
    "    \"\"\"\n",
    "    Ghi checkpoint an to√†n cho m·ªói batch.\n",
    "    \"\"\"\n",
    "    new_state = {\n",
    "        'last_id': last_id,\n",
    "        'section_time': section_time,\n",
    "        'total_time': total_time\n",
    "    }\n",
    "    save_state(checkpoint_folder / f'metadata_checkpoint_{checkpoint_id}', new_state)\n",
    "    print(f\"Saved checkpoint {checkpoint_id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "43237f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_arXiv_metadata(start_month: str, start_id: int,\n",
    "                         end_month: str, end_id: int,\n",
    "                         batch_size: int = 100,\n",
    "                         delay: float = 0.125,\n",
    "                         query_delay: float = 3,\n",
    "                         retries: int = 3,\n",
    "                         folder: Optional[Path] = None):\n",
    "    \"\"\"Crawl metadata arXiv using arxiv.py by ID, for all version (1 file per paper ID).\"\"\"\n",
    "\n",
    "    start_yymm = int(month_to_yymm(start_month))\n",
    "    end_yymm = int(month_to_yymm(end_month))\n",
    "\n",
    "    # Load checkpoint\n",
    "    resume = load_checkpoint_state(folder, end_yymm, end_id)\n",
    "    if resume == (None, None, None, None, None):\n",
    "        return\n",
    "    if resume[0] is not None:\n",
    "        start_yymm, id_num, checkpoint_id, total_time, _ = resume\n",
    "    else:\n",
    "        id_num, checkpoint_id, total_time = start_id, 1, 0\n",
    "\n",
    "    # T·∫°o danh s√°ch ID c·∫ßn crawl\n",
    "    id_list = build_id_list(start_yymm, id_num, end_yymm, end_id)\n",
    "    client = arxiv.Client(page_size=batch_size, delay_seconds=delay, num_retries=retries)\n",
    "\n",
    "    curr_batch_start = 0\n",
    "    yymm = start_yymm\n",
    "\n",
    "    while curr_batch_start < len(id_list):\n",
    "        batch_ids = id_list[curr_batch_start: curr_batch_start + batch_size]\n",
    "        curr_batch_start += batch_size\n",
    "\n",
    "        search = arxiv.Search(id_list=batch_ids)\n",
    "        successful_crawls = 0\n",
    "        time_start = time.time()\n",
    "\n",
    "        # Crawl song song metadata t·ª´ng b√†i\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futures = []\n",
    "            for result in client.results(search):\n",
    "                # üîß S·ª≠a ·ªü ƒë√¢y: ch·ªâ t·∫°o th∆∞ m·ª•c ID g·ªëc, kh√¥ng t·∫°o theo version\n",
    "                base_id = result.get_short_id().split('v')[0]\n",
    "                meta_folder = folder / base_id.replace(\".\", \"-\")\n",
    "                meta_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Ghi duy nh·∫•t 1 file metadata.json\n",
    "                meta_path = meta_folder / \"metadata.json\"\n",
    "\n",
    "                # N·∫øu file ƒë√£ t·ªìn t·∫°i, b·ªè qua (tr√°nh ghi ƒë√®)\n",
    "                if meta_path.exists():\n",
    "                    print(f\"Metadata exists for {base_id}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # G·ª≠i nhi·ªám v·ª• t·ªõi worker (fetch_metadata_worker)\n",
    "                futures.append(\n",
    "                    ex.submit(fetch_metadata_worker, result, client, meta_folder, query_delay)\n",
    "                )\n",
    "\n",
    "            for fut in futures:\n",
    "                try:\n",
    "                    successful_crawls += fut.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Metadata fetch failed: {e}\")\n",
    "\n",
    "        time_spent = time.time() - time_start\n",
    "\n",
    "        # L∆∞u checkpoint n·∫øu batch th√†nh c√¥ng\n",
    "        if successful_crawls == len(batch_ids):\n",
    "            total_time += time_spent\n",
    "            save_checkpoint(WORK_DIR, checkpoint_id, batch_ids[-1], time_spent, total_time)\n",
    "            checkpoint_id += 1\n",
    "\n",
    "        elif successful_crawls == 0 and yymm < end_yymm:\n",
    "            print(\"Metadata list is empty. Moving to next month...\")\n",
    "            curr_batch_start = 0\n",
    "            if yymm % 100 == 12:\n",
    "                yymm += 88\n",
    "            yymm += 1\n",
    "            id_list = id_list[id_list.index(f\"{yymm}.00001\"):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "9573ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_version_list_from_metadata(metadata_dir: Path):\n",
    "    \"\"\"ƒê·ªçc to√†n b·ªô metadata.json trong c√°c th∆∞ m·ª•c con ƒë·ªÉ thu ƒë∆∞·ª£c danh s√°ch arXiv version IDs.\"\"\"\n",
    "    version_list = []\n",
    "\n",
    "    for subdir in metadata_dir.iterdir():\n",
    "        if subdir.is_dir():\n",
    "            meta_file = subdir / \"metadata.json\"\n",
    "            if meta_file.exists():\n",
    "                with open(meta_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                arxiv_id = data.get(\"id\", \"\")\n",
    "                if arxiv_id:\n",
    "                    # N·∫øu b√†i c√≥ nhi·ªÅu phi√™n b·∫£n -> th√™m ƒë·ªß c√°c version\n",
    "                    if \"revised_dates\" in data:\n",
    "                        n_ver = len(data[\"revised_dates\"])\n",
    "                        for v in range(1, n_ver + 1):\n",
    "                            version_list.append(f\"{arxiv_id}v{v}\")\n",
    "                    else:\n",
    "                        version_list.append(f\"{arxiv_id}v1\")\n",
    "\n",
    "    print(f\"Total versions to download: {len(version_list)}\")\n",
    "    return version_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bdec0e",
   "metadata": {},
   "source": [
    "## **Crawl paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "a4484780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_gzip_valid(path: Path):\n",
    "    \"\"\"\n",
    "    Ki·ªÉm tra file .gz c√≥ h·ª£p l·ªá kh√¥ng.\n",
    "    \"\"\"\n",
    "    if not path.exists() or path.stat().st_size == 0:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # Check GZIP integrity\n",
    "        with gzip.open(str(path), \"rb\") as gz:\n",
    "            while gz.read(1024 * 1024):\n",
    "                pass\n",
    "\n",
    "        return True\n",
    "    except (IOError, EOFError, gzip.BadGzipFile):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "668d4282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_tar_gz_valid(path: Path):\n",
    "    \"\"\"\n",
    "    Ki·ªÉm tra file .tar.gz c√≥ h·ª£p l·ªá kh√¥ng.\n",
    "    \"\"\"\n",
    "    if not path.exists() or path.stat().st_size == 0:\n",
    "        return False\n",
    "\n",
    "    if not tarfile.is_tarfile(str(path)):\n",
    "        return False\n",
    "    if not is_gzip_valid(path):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "5d6d890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_filetype(path: Path):\n",
    "    kind = filetype.guess(path)\n",
    "    if kind:\n",
    "        return kind.extension, f\"{kind.mime} (by filetype)\"\n",
    "    return None, \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "680aeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_paper_source_worker(result: arxiv.Result,\n",
    "                              client: arxiv.Client,\n",
    "                              output_folder: Path,\n",
    "                              papers_dir: Path,\n",
    "                              query_delay: float,\n",
    "                              failed_list: List[str]):\n",
    "    curr_id = result.get_short_id()\n",
    "\n",
    "    # Ki·ªÉm tra file ƒë√£ t·∫£i v√† h·ª£p l·ªá\n",
    "    tar_path = output_folder / f\"{curr_id}.tar.gz\"\n",
    "    if tar_path.exists() and is_tar_gz_valid(tar_path):\n",
    "        print(f\"{curr_id}.tar.gz is already downloaded.\")\n",
    "        return\n",
    "\n",
    "    # N·∫øu t·ªìn t·∫°i nh∆∞ng h·ªèng th√¨ x√≥a\n",
    "    elif tar_path.exists():\n",
    "        os.remove(tar_path)\n",
    "\n",
    "    # Ki·ªÉm tra file c√≥ ph·∫ßn m·ªü r·ªông kh√°c\n",
    "    else:\n",
    "        try:\n",
    "            actual_file_name = list(output_folder.glob(f\"{curr_id}.*\"))[0]\n",
    "            print(f\"{actual_file_name.name} is already downloaded.\")\n",
    "            return\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    # Ti·∫øn h√†nh t·∫£i\n",
    "    try:\n",
    "        print(f\"Start downloading source for arXiv ID {curr_id}.\")\n",
    "        result.download_source(str(output_folder), filename=f\"{curr_id}.tar.gz\")\n",
    "\n",
    "        # X√°c ƒë·ªãnh lo·∫°i file th·∫≠t s·ª±\n",
    "        tar_full_path = papers_dir / f\"{curr_id}.tar.gz\"\n",
    "        suitable_ext, _ = detect_with_filetype(tar_full_path)\n",
    "\n",
    "        if suitable_ext and suitable_ext != \"gz\":\n",
    "            os.rename(tar_full_path, papers_dir / f\"{curr_id}.{suitable_ext}\")\n",
    "\n",
    "        elif not is_tar_gz_valid(tar_full_path):\n",
    "            os.rename(tar_full_path, papers_dir / f\"{curr_id}.gz\")\n",
    "\n",
    "        print(f\"Finished downloading source for arXiv ID {curr_id}.\")\n",
    "        time.sleep(query_delay)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {curr_id} (.tar.gz). Error: {e}\")\n",
    "        if curr_id not in failed_list:\n",
    "            failed_list.append(curr_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "7b1608cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_paper_source(version_list: List[str],\n",
    "                       output_folder: Path,\n",
    "                       batch_size: int = 100,\n",
    "                       delay: float = 0.125,\n",
    "                       query_delay: float = 3,\n",
    "                       retries: int = 3):\n",
    "    \n",
    "    print(f\"Total versions to download after filtering existing valid files: {len(version_list)}\")\n",
    "\n",
    "    failed_list = []\n",
    "    client = arxiv.Client(page_size=batch_size, delay_seconds=delay, num_retries=retries)\n",
    "    papers_dir = output_folder  # ho·∫∑c Path(\"../bulk\") n·∫øu b·∫°n mu·ªën ƒë·ªÉ ri√™ng\n",
    "\n",
    "    curr_batch_start = 0\n",
    "    while curr_batch_start < len(version_list):\n",
    "        batch_ids = version_list[curr_batch_start: curr_batch_start + batch_size]\n",
    "        curr_batch_start += batch_size\n",
    "        print(f\"Downloading batch starting at {batch_ids[0]}...\")\n",
    "\n",
    "        search = arxiv.Search(id_list=batch_ids)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futures = [\n",
    "                ex.submit(\n",
    "                    fetch_paper_source_worker,\n",
    "                    result,\n",
    "                    client,\n",
    "                    output_folder,\n",
    "                    papers_dir,\n",
    "                    query_delay,\n",
    "                    failed_list\n",
    "                )\n",
    "                for result in client.results(search)\n",
    "            ]\n",
    "\n",
    "            # ƒë·ª£i t·∫•t c·∫£ ho√†n th√†nh\n",
    "            for fut in futures:\n",
    "                fut.result()\n",
    "\n",
    "    return failed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "a15f42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl_paper_source(version_list, papers_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "ebdd14b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file found for 2211.13747v1\n",
      "No file found for 2211.13748v1\n",
      "No file found for 2211.13749v1\n",
      "No file found for 2211.13750v1\n",
      "No file found for 2211.13750v2\n",
      "No file found for 2211.13751v1\n",
      "No file found for 2211.13752v1\n",
      "No file found for 2211.13753v1\n",
      "No file found for 2211.13754v1\n",
      "No file found for 2211.13755v1\n",
      "No file found for 2211.13755v2\n",
      "\n",
      "Summary:\n",
      "0 successful\n",
      "11 failed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "successful_downloads = []\n",
    "failed_downloads = []\n",
    "\n",
    "for ver in version_list:\n",
    "    try:\n",
    "        # L·∫•y t·∫•t c·∫£ c√°c file c√≥ c√πng ID\n",
    "        candidates = list(papers_dir.glob(f\"{ver}.*\"))\n",
    "        if not candidates:\n",
    "            print(f\"No file found for {ver}\")\n",
    "            failed_downloads.append(ver)\n",
    "            continue\n",
    "\n",
    "        found_success = False\n",
    "        for f in candidates:\n",
    "            name = f.name.lower()\n",
    "\n",
    "            # ---- Tr∆∞·ªùng h·ª£p 1: tar.gz ----\n",
    "            if name.endswith(\".tar.gz\"):\n",
    "                if is_tar_gz_valid(f):\n",
    "                    print(f\"Valid tar.gz: {f.name}\")\n",
    "                    successful_downloads.append(ver)\n",
    "                    found_success = True\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Corrupted tar.gz: {f.name}\")\n",
    "\n",
    "            # ---- Tr∆∞·ªùng h·ª£p 2: gz ----\n",
    "            elif name.endswith(\".gz\"):\n",
    "                if is_gzip_valid(f):\n",
    "                    print(f\"Valid gzip: {f.name}\")\n",
    "                    successful_downloads.append(ver)\n",
    "                    found_success = True\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Corrupted gzip: {f.name}\")\n",
    "\n",
    "            # ---- Tr∆∞·ªùng h·ª£p 3: pdf ----\n",
    "            elif name.endswith(\".pdf\"):\n",
    "                print(f\"Found PDF: {f.name}\")\n",
    "                successful_downloads.append(ver)\n",
    "                found_success = True\n",
    "                break\n",
    "\n",
    "        if not found_success:\n",
    "            print(f\"‚ùå All files for {ver} invalid or unreadable.\")\n",
    "            failed_downloads.append(ver)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error while checking {ver}: {e}\")\n",
    "        failed_downloads.append(ver)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"{len(successful_downloads)} successful\")\n",
    "print(f\"{len(failed_downloads)} failed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "767a049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('failed_downloaded_ids.txt', 'w') as f:\n",
    "    for id in failed_downloads:\n",
    "        f.write(id + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "accc448a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(successful_downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "8089f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successful_downloads[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "89bcbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl_paper_source(failed_downloads, \"..\" / papers_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "48bb593e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_ids = []\n",
    "for ver in version_list:\n",
    "    if ver.find('v1') > -1:\n",
    "        original_ids.append(ver[:ver.find('v1')])\n",
    "\n",
    "len(original_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "8b3baaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_download_list = [str(file) for file in list((papers_dir).glob(\"*.*\"))]\n",
    "\n",
    "len(actual_download_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "55188dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gz_list = [str(file) for file in list((papers_dir).glob(\"*.gz\"))]\n",
    "pdf_list = [str(file) for file in list((papers_dir).glob(\"*.pdf\"))]\n",
    "tar_gz_list = []\n",
    "non_tar_gz_list = []\n",
    "for file in gz_list:\n",
    "    if file.find('.tar') > -1:\n",
    "        tar_gz_list.append(file)\n",
    "    else:\n",
    "        non_tar_gz_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "0598ef95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(pdf_list))\n",
    "print(len(tar_gz_list))\n",
    "print(len(non_tar_gz_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "4ae11d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_download_ids = ['.'.join(file.split('/')[-1].split('.')[:2]) for file in actual_download_list]\n",
    "failed_download_ids = []\n",
    "for id in version_list:\n",
    "    if id not in actual_download_ids:\n",
    "        failed_download_ids.append(id)\n",
    "\n",
    "len(failed_download_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "878e089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl_paper_source(failed_download_ids, papers_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "843297c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata_status(\n",
    "    metadata_dir: Path,\n",
    "    version_list: list,\n",
    "    actual_download_list: list,\n",
    "    papers_dir: Path,\n",
    "    output_dir: Path):\n",
    "    metadata_status = []\n",
    "\n",
    "    # Duy·ªát to√†n b·ªô c√°c file metadata.json n·∫±m trong c√°c th∆∞ m·ª•c con\n",
    "    for meta_file in metadata_dir.rglob(\"metadata.json\"):\n",
    "        try:\n",
    "            with open(meta_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "\n",
    "            arxiv_id = metadata.get(\"id\", None)\n",
    "            if not arxiv_id:\n",
    "                continue\n",
    "\n",
    "            # Ki·ªÉm tra xem file paper t∆∞∆°ng ·ª©ng c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "            matched = [file for file in actual_download_list if arxiv_id in file]\n",
    "            if matched:\n",
    "                metadata[\"paper_status\"] = \"available\"\n",
    "            else:\n",
    "                metadata[\"paper_status\"] = \"missing\"\n",
    "\n",
    "            metadata_status.append(metadata)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è L·ªói khi ƒë·ªçc {meta_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # ƒê·∫£m b·∫£o th∆∞ m·ª•c ƒë√≠ch t·ªìn t·∫°i\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = output_dir / \"metadata_status.json\"\n",
    "\n",
    "    # Ghi file k·∫øt qu·∫£\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata_status, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"File metadata_status.json ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "#generate_metadata_status(metadatas_dir, version_list, actual_download_list, papers_dir, WORK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d6c59",
   "metadata": {},
   "source": [
    "## **Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "b2cf3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_TEXT   = 0x01\n",
    "F_HCRC   = 0x02\n",
    "F_COMMENT= 0x10\n",
    "_FORBIDDEN = set('/\\0')  # we treat tar paths as POSIX; '/' splits components\n",
    "_CTRL_RANGES = [(0x00, 0x1F), (0x7F, 0x9F)]\n",
    "_SURR_MIN, _SURR_MAX = 0xD800, 0xDFFF\n",
    "_MAX_COMP_LEN = 200  # keep some headroom under 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "ee9056b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_control(ch: str) -> bool:\n",
    "    cp = ord(ch)\n",
    "    return any(lo <= cp <= hi for lo, hi in _CTRL_RANGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "2ca473d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_component(comp: str) -> str:\n",
    "    # Normalize to NFC\n",
    "    comp = unicodedata.normalize('NFC', comp)\n",
    "\n",
    "    # Remove path separators and NULs early\n",
    "    comp = ''.join('_' if c in _FORBIDDEN else c for c in comp)\n",
    "\n",
    "    # Drop/control-map and surrogate-map\n",
    "    cleaned = []\n",
    "    for c in comp:\n",
    "        o = ord(c)\n",
    "        if _is_control(c):\n",
    "            cleaned.append('_')\n",
    "        elif _SURR_MIN <= o <= _SURR_MAX:\n",
    "            cleaned.append('_')  # unpaired surrogate ‚Üí safe underscore\n",
    "        else:\n",
    "            cleaned.append(c)\n",
    "    comp = ''.join(cleaned).strip()\n",
    "\n",
    "    # Collapse empty/dots\n",
    "    if comp in ('', '.', '..'):\n",
    "        comp = '_'\n",
    "\n",
    "    # Avoid leading '-' (we might pass names to tools later)\n",
    "    if comp.startswith('-'):\n",
    "        comp = '_' + comp[1:]\n",
    "\n",
    "    # Trim very long components while trying to keep extension\n",
    "    if len(comp) > _MAX_COMP_LEN:\n",
    "        stem, dot, ext = comp.partition('.')\n",
    "        if dot:  # has an extension\n",
    "            stem = stem[:_MAX_COMP_LEN - 1 - len(ext)]\n",
    "            comp = f\"{stem}.{ext}\"\n",
    "        else:\n",
    "            comp = comp[:_MAX_COMP_LEN]\n",
    "\n",
    "    return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "81d570d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_relative_path(posix_path: str) -> str:\n",
    "    # Split on POSIX '/', sanitize each piece, then rejoin with os.sep-neutral join below\n",
    "    parts = [p for p in posix_path.split('/') if p not in ('',)]\n",
    "    san = [_sanitize_component(p) for p in parts]\n",
    "    # Prevent accidental absolute/parent after sanitization\n",
    "    san = ['_' if p in ('', '.', '..') else p for p in san]\n",
    "    clean_rel = '/'.join(san)\n",
    "    if clean_rel != posix_path:\n",
    "        print(f\"[sanitize] {posix_path!r} -> {clean_rel!r}\")\n",
    "    return clean_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "dce372ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dedupe_path(target: Path, original_hint: str) -> Path:\n",
    "    \"\"\"If 'target' exists, append a short hash derived from the original name.\"\"\"\n",
    "    if not target.exists():\n",
    "        return target\n",
    "    stem = target.stem\n",
    "    suffix = target.suffix\n",
    "    h = hashlib.sha1(original_hint.encode('utf-8', 'surrogatepass')).hexdigest()[:8]\n",
    "    cand = target.with_name(f\"{stem}__{h}{suffix}\")\n",
    "    i = 1\n",
    "    while cand.exists():\n",
    "        cand = target.with_name(f\"{stem}__{h}_{i}{suffix}\")\n",
    "        i += 1\n",
    "    return cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "5fb28707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_gzip_original_name(path: Path) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Parse the gzip header to recover the original filename (FNAME) if present.\n",
    "    Returns a sanitized base name (no directories). If absent, returns None.\n",
    "    \"\"\"\n",
    "    with path.open(\"rb\") as f:\n",
    "        data = f.read(10)  # ID1 ID2 CM FLG MTIME(4) XFL OS\n",
    "        if len(data) < 10:\n",
    "            return None\n",
    "        # GZIP_MAGIC: b\"\\x1f\\x8b\"\n",
    "        if data[0:2] != b\"\\x1f\\x8b\":\n",
    "            return None\n",
    "\n",
    "        cm = data[2]\n",
    "        flg = data[3]\n",
    "        # F_RESERVED: 0xE0\n",
    "        if cm != 8 or (flg & 0xE0):\n",
    "            return None\n",
    "\n",
    "        # After the fixed 10-byte header:\n",
    "        # Optional sections in order: EXTRA, NAME, COMMENT, HCRC\n",
    "        def _skip(n: int):\n",
    "            f.seek(n, io.SEEK_CUR)\n",
    "\n",
    "        # EXTRA\n",
    "        # F_EXTRA: 0x04\n",
    "        if flg & 0x04:\n",
    "            xtra_len_bytes = f.read(2)\n",
    "            if len(xtra_len_bytes) != 2:\n",
    "                return None\n",
    "            xlen = struct.unpack(\"<H\", xtra_len_bytes)[0]\n",
    "            _skip(xlen)\n",
    "\n",
    "        # NAME (zero-terminated)\n",
    "        original_name = None\n",
    "        # F_NAME: 0x08\n",
    "        if flg & 0x08:\n",
    "            name_bytes = bytearray()\n",
    "            while True:\n",
    "                b = f.read(1)\n",
    "                if not b:\n",
    "                    # Unexpected EOF\n",
    "                    break\n",
    "                if b == b\"\\x00\":\n",
    "                    break\n",
    "                name_bytes.extend(b)\n",
    "            try:\n",
    "                raw = name_bytes.decode(\"latin-1\", errors=\"replace\")\n",
    "                # keep only basename and sanitize for the filesystem\n",
    "                base = os.path.basename(raw)\n",
    "                original_name = _sanitize_component(base)\n",
    "            except Exception:\n",
    "                original_name = None\n",
    "\n",
    "        # We don't need COMMENT or HCRC for name extraction\n",
    "\n",
    "        if original_name:\n",
    "            # Return only the final component to avoid path traversal\n",
    "            return os.path.basename(original_name)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "16380bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_join(base: Path, *paths: str) -> Path:\n",
    "    \"\"\"\n",
    "    Join paths and ensure the result stays within base.\n",
    "    Prevents path traversal attacks.\n",
    "    \"\"\"\n",
    "    base = base.resolve()\n",
    "    final = base\n",
    "    for p in paths:\n",
    "        final = final / str(p)\n",
    "    final = final.resolve()\n",
    "    if not (str(final) == str(base) or str(final).startswith(str(base) + os.sep)):\n",
    "        raise ValueError(f\"Blocked path traversal attempt: {final}\")\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "6faf81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_extract_tar(tar: tarfile.TarFile, dest: Path, members: Optional[Iterable[tarfile.TarInfo]] = None):\n",
    "    \"\"\"\n",
    "    Secure tar extraction: ensure no member escapes dest and filenames are macOS-safe.\n",
    "    Skips symlinks and special files.\n",
    "    \"\"\"\n",
    "    for m in (members or tar):\n",
    "        # Skip unsafe types\n",
    "        if m.islnk() or m.issym() or m.ischr() or m.isblk() or m.isfifo():\n",
    "            continue\n",
    "\n",
    "        # Sanitize relative path (tar uses POSIX '/')\n",
    "        clean_rel = _sanitize_relative_path(m.name)\n",
    "        if clean_rel == '':\n",
    "            continue\n",
    "\n",
    "        # Join & ensure containment\n",
    "        target_path = _safe_join(dest, clean_rel)\n",
    "\n",
    "        # De-duplicate on collision\n",
    "        target_path = _dedupe_path(target_path, m.name)\n",
    "\n",
    "        if m.isdir():\n",
    "            target_path.mkdir(parents=True, exist_ok=True)\n",
    "            # best-effort permissions on dirs\n",
    "            try:\n",
    "                os.chmod(target_path, m.mode)\n",
    "            except Exception:\n",
    "                pass\n",
    "            continue\n",
    "\n",
    "        # Ensure parent exists\n",
    "        target_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Extract regular file\n",
    "        src = tar.extractfile(m)\n",
    "        if src is None:\n",
    "            continue  # skip specials with no extractable content\n",
    "        with src, open(target_path, \"wb\") as out:\n",
    "            # Stream copy in chunks\n",
    "            while True:\n",
    "                chunk = src.read(1024 * 1024)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                out.write(chunk)\n",
    "\n",
    "        # Preserve permissions (best-effort)\n",
    "        try:\n",
    "            os.chmod(target_path, m.mode)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "8394458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_archive(path: Path, out_dir: Path) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Gi·∫£i n√©n m·ªçi lo·∫°i tar archive (tar, tar.gz, tar.xz, v.v.)\n",
    "    v√†o th∆∞ m·ª•c out_dir. ƒê·∫£m b·∫£o an to√†n t√™n file, kh√¥ng path traversal.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    extracted: list[Path] = []\n",
    "\n",
    "    with tarfile.open(str(path), mode=\"r:*\") as tar:\n",
    "        members = tar.getmembers()\n",
    "        _safe_extract_tar(tar, out_dir, members)\n",
    "\n",
    "        for m in members:\n",
    "            clean_rel = _sanitize_relative_path(m.name)\n",
    "            if not clean_rel:\n",
    "                continue\n",
    "            target = (out_dir / clean_rel).resolve()\n",
    "            if target.exists():\n",
    "                extracted.append(target)\n",
    "\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "14143482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_plain_gzip(path: Path, out_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Gi·∫£i n√©n file GZIP (.gz) ƒë∆°n l·∫ª v√†o th∆∞ m·ª•c out_dir.\n",
    "    N·∫øu file l√† .tar.gz, k·∫øt qu·∫£ v·∫´n gi·ªØ d·∫°ng .tar (cho v√≤ng sau).\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    header_name = _read_gzip_original_name(path)\n",
    "    if header_name:\n",
    "        out_name = header_name\n",
    "    else:\n",
    "        if path.name.lower().endswith(\".gz\") and len(path.name) > 3:\n",
    "            out_name = path.name[:-3]\n",
    "        else:\n",
    "            out_name = path.name + \".out\"\n",
    "\n",
    "    out_name = _sanitize_component(os.path.basename(out_name)) or \"_\"\n",
    "    out_path = _safe_join(out_dir, out_name)\n",
    "    out_path = _dedupe_path(out_path, out_name)\n",
    "\n",
    "    with gzip.open(path, \"rb\") as gz, open(out_path, \"wb\") as out:\n",
    "        for chunk in iter(lambda: gz.read(1024 * 1024), b\"\"):\n",
    "            out.write(chunk)\n",
    "\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "6e6b8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_until_done(path: Path, out_dir: Path, max_depth: int = 5):\n",
    "    \"\"\"\n",
    "    ƒê·ªá quy gi·∫£i n√©n file nhi·ªÅu l·ªõp (tar, tar.gz, tgz, .gz...),\n",
    "    lu√¥n gi·∫£i n√©n v√†o th∆∞ m·ª•c out_dir (kh√¥ng ·ªü c·∫°nh file g·ªëc).\n",
    "    \"\"\"\n",
    "    extracted_files = []\n",
    "    current_files = [path]\n",
    "    depth = 0\n",
    "\n",
    "    while current_files and depth < max_depth:\n",
    "        next_files = []\n",
    "        for f in current_files:\n",
    "            try:\n",
    "                # ---- N·∫øu l√† TAR ----\n",
    "                if tarfile.is_tarfile(f):\n",
    "                    print(f\"[{depth}] Extracting TAR archive: {f}\")\n",
    "                    new_files = extract_tar_archive(f, out_dir)\n",
    "                    extracted_files.extend(new_files)\n",
    "                    next_files.extend(new_files)\n",
    "                    continue\n",
    "\n",
    "                # ---- N·∫øu l√† GZIP ----\n",
    "                elif f.suffix == \".gz\":\n",
    "                    print(f\"[{depth}] Extracting GZIP file: {f}\")\n",
    "                    new_file = extract_plain_gzip(f, out_dir)\n",
    "                    extracted_files.append(new_file)\n",
    "                    next_files.append(new_file)\n",
    "                    continue\n",
    "\n",
    "                # ---- Kh√¥ng gi·∫£i ƒë∆∞·ª£c (b·ªè qua) ----\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Kh√¥ng th·ªÉ gi·∫£i n√©n {f.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # L·ªçc file m·ªõi sinh ra ƒë·ªÉ xem c√≥ c·∫ßn gi·∫£i ti·∫øp kh√¥ng\n",
    "        current_files = [nf for nf in next_files if nf.suffix in [\".gz\", \".xz\", \".tar\", \".tgz\"]]\n",
    "        depth += 1\n",
    "\n",
    "    if depth >= max_depth:\n",
    "        print(\"D·ª´ng l·∫°i do ƒë·∫°t gi·ªõi h·∫°n ƒë·ªô s√¢u (max_depth).\")\n",
    "    else:\n",
    "        print(\"Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\")\n",
    "\n",
    "    return extracted_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d47dd",
   "metadata": {},
   "source": [
    "## **Delete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "043e17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_include_graphics(tex_path: Path):\n",
    "    \"\"\"Th√™m comment tr∆∞·ªõc c√°c d√≤ng includegraphics ƒë·ªÉ tr√°nh l·ªói khi compile.\"\"\"\n",
    "    if not tex_path.is_file():\n",
    "        return\n",
    "    with open(tex_path, 'r', encoding='latin-1') as f:\n",
    "        tex_source = f.readlines()\n",
    "\n",
    "    graphics_count = 0\n",
    "    for i, line in enumerate(tex_source):\n",
    "        if '\\\\includegraphics' in line and '%' not in line:\n",
    "            tex_source[i] = line[:line.find('\\\\includegraphics')] + '%' + line[line.find('\\\\includegraphics'):]\n",
    "            graphics_count += 1\n",
    "\n",
    "    if graphics_count > 0:\n",
    "        print(f\"Commented {graphics_count} includegraphics in {tex_path.name}\")\n",
    "\n",
    "    with open(tex_path, 'w', encoding='latin-1') as f:\n",
    "        f.writelines(tex_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "fb8e8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_image_files(root: Path):\n",
    "    \"\"\"X√≥a to√†n b·ªô file h√¨nh ·∫£nh trong th∆∞ m·ª•c.\"\"\"\n",
    "    exts = {\".png\", \".jpg\", \".jpeg\", \".pdf\", \".eps\", \".gif\", \".tif\", \".tiff\", \".bmp\", \".svg\"}\n",
    "    count = 0\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in exts:\n",
    "            try:\n",
    "                p.unlink()\n",
    "                count += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "    if count:\n",
    "        print(f\"Removed {count} image files from {root.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "d90d4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_tex_files(root: Path, keep_exts=None):\n",
    "    \"\"\"Gi·ªØ l·∫°i .tex v√† .bib, x√≥a to√†n b·ªô file c√≤n l·∫°i.\"\"\"\n",
    "    if keep_exts is None:\n",
    "        keep_exts = {\".tex\", \".bib\"}\n",
    "\n",
    "    if not root.exists():\n",
    "        print(f\"‚ö†Ô∏è Th∆∞ m·ª•c {root} kh√¥ng t·ªìn t·∫°i.\")\n",
    "        return\n",
    "\n",
    "    count_deleted, count_kept = 0, 0\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            ext = p.suffix.lower()\n",
    "            if ext not in keep_exts:\n",
    "                try:\n",
    "                    p.unlink()\n",
    "                    count_deleted += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "            else:\n",
    "                count_kept += 1\n",
    "\n",
    "    print(f\"D·ªçn d·∫πp '{root.name}': gi·ªØ {count_kept}, x√≥a {count_deleted} file kh√°c.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "4e884070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_empty_dirs(root: Path):\n",
    "    \"\"\"ƒê·ªá quy x√≥a t·∫•t c·∫£ th∆∞ m·ª•c r·ªóng.\"\"\"\n",
    "    count = 0\n",
    "    for p in sorted(root.rglob(\"*\"), reverse=True):\n",
    "        if p.is_dir() and not any(p.iterdir()):\n",
    "            try:\n",
    "                p.rmdir()\n",
    "                count += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "    if count:\n",
    "        print(f\"ƒê√£ x√≥a {count} th∆∞ m·ª•c r·ªóng trong '{root.name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "8394e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_flatten_subdirs(paper_dir: Path):\n",
    "    \"\"\"\n",
    "    D·ªçn d·∫πp th∆∞ m·ª•c paper sao cho:\n",
    "    - N·∫øu th∆∞ m·ª•c con ch·ª©a file .tex/.bib KH√ÅC th∆∞ m·ª•c cha ‚Üí d·ªùi ra ngo√†i r·ªìi x√≥a subdir.\n",
    "    - N·∫øu th∆∞ m·ª•c con ch·ª©a file .tex/.bib GI·ªêNG th∆∞ m·ª•c cha ‚Üí x√≥a subdir.\n",
    "    - N·∫øu kh√¥ng c√≥ file .tex/.bib ‚Üí x√≥a subdir lu√¥n.\n",
    "    \"\"\"\n",
    "    if not paper_dir.exists() or not paper_dir.is_dir():\n",
    "        return\n",
    "\n",
    "    tex_bib_exts = {\".tex\", \".bib\"}\n",
    "    moved, deleted = 0, 0\n",
    "    parent_files = {p.name for p in paper_dir.glob(\"*\") if p.suffix.lower() in tex_bib_exts}\n",
    "\n",
    "    for subdir in [d for d in paper_dir.iterdir() if d.is_dir()]:\n",
    "        sub_files = {p.name for p in subdir.rglob(\"*\") if p.suffix.lower() in tex_bib_exts}\n",
    "\n",
    "        # N·∫øu kh√¥ng c√≥ file .tex/.bib ‚Üí x√≥a lu√¥n\n",
    "        if not sub_files:\n",
    "            shutil.rmtree(subdir, ignore_errors=True)\n",
    "            deleted += 1\n",
    "            continue\n",
    "\n",
    "        # N·∫øu tr√πng ho√†n to√†n ‚Üí x√≥a\n",
    "        if sub_files == parent_files:\n",
    "            shutil.rmtree(subdir, ignore_errors=True)\n",
    "            deleted += 1\n",
    "        else:\n",
    "            # Kh√°c ‚Üí d·ªùi file ra ngo√†i r·ªìi x√≥a\n",
    "            for f in subdir.rglob(\"*\"):\n",
    "                if f.is_file() and f.suffix.lower() in tex_bib_exts:\n",
    "                    dest = paper_dir / f.name\n",
    "                    if not dest.exists():\n",
    "                        shutil.move(str(f), str(dest))\n",
    "                        moved += 1\n",
    "            shutil.rmtree(subdir, ignore_errors=True)\n",
    "            deleted += 1\n",
    "\n",
    "    print(f\"D·ªçn {paper_dir.name}: di chuy·ªÉn {moved} file, x√≥a {deleted} th∆∞ m·ª•c con.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "a58c05e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor file in actual_download_list:\\n    file_path = Path(file)\\n    id = \\'.\\'.join(file_path.stem.split(\\'.\\')[:2])\\n    target_dir = (WORK_DIR / \"extracted\" / id).resolve()\\n    target_dir.mkdir(parents=True, exist_ok=True)\\n\\n    print(f\"\\nDecompressing {file_path.name} ‚Üí {target_dir}\")\\n\\n    try:\\n        extract_until_done(file_path, target_dir, max_depth=5)\\n        print(f\"Done extracting {file_path.name}\")\\n\\n        # üîπ L√†m ph·∫≥ng & gom file .tex/.bib h·ª£p l·ªá\\n        clean_and_flatten_subdirs(target_dir)\\n\\n        # üîπ Comment d√≤ng includegraphics trong .tex\\n        for tex_file in target_dir.rglob(\"*.tex\"):\\n            comment_include_graphics(tex_file)\\n\\n        # üîπ X√≥a ·∫£nh, file th·ª´a, th∆∞ m·ª•c r·ªóng\\n        delete_image_files(target_dir)\\n        delete_non_tex_files(target_dir)\\n        delete_empty_dirs(target_dir)\\n\\n        print(f\"Ho√†n t·∫•t d·ªçn th∆∞ m·ª•c: {target_dir.name}\")\\n\\n    except Exception as e:\\n        print(f\"L·ªói khi gi·∫£i n√©n {file_path.name}: {e}\")\\n        try:\\n            shutil.copy2(file_path, target_dir)\\n            print(f\"Copied {file_path.name} v√†o {target_dir}\")\\n        except Exception as copy_err:\\n            print(f\"Kh√¥ng th·ªÉ copy {file_path.name}: {copy_err}\")\\n'"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for file in actual_download_list:\n",
    "    file_path = Path(file)\n",
    "    id = '.'.join(file_path.stem.split('.')[:2])\n",
    "    target_dir = (WORK_DIR / \"extracted\" / id).resolve()\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nDecompressing {file_path.name} ‚Üí {target_dir}\")\n",
    "\n",
    "    try:\n",
    "        extract_until_done(file_path, target_dir, max_depth=5)\n",
    "        print(f\"Done extracting {file_path.name}\")\n",
    "\n",
    "        # üîπ L√†m ph·∫≥ng & gom file .tex/.bib h·ª£p l·ªá\n",
    "        clean_and_flatten_subdirs(target_dir)\n",
    "\n",
    "        # üîπ Comment d√≤ng includegraphics trong .tex\n",
    "        for tex_file in target_dir.rglob(\"*.tex\"):\n",
    "            comment_include_graphics(tex_file)\n",
    "\n",
    "        # üîπ X√≥a ·∫£nh, file th·ª´a, th∆∞ m·ª•c r·ªóng\n",
    "        delete_image_files(target_dir)\n",
    "        delete_non_tex_files(target_dir)\n",
    "        delete_empty_dirs(target_dir)\n",
    "\n",
    "        print(f\"Ho√†n t·∫•t d·ªçn th∆∞ m·ª•c: {target_dir.name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi gi·∫£i n√©n {file_path.name}: {e}\")\n",
    "        try:\n",
    "            shutil.copy2(file_path, target_dir)\n",
    "            print(f\"Copied {file_path.name} v√†o {target_dir}\")\n",
    "        except Exception as copy_err:\n",
    "            print(f\"Kh√¥ng th·ªÉ copy {file_path.name}: {copy_err}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b6927",
   "metadata": {},
   "source": [
    "## **Multi-thread**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b443d5f",
   "metadata": {},
   "source": [
    "### **4 lu·ªìng song song**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "4e5950e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\\nfrom queue import Queue\\nfrom threading import Thread\\nimport shutil, time\\n\\ndef download_version(arxiv_id, client, output_dir, query_delay):\\n    #T·∫£i 1 version paper t·ª´ arXiv v√† tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n file .tar.gz\\n    try:\\n        search = arxiv.Search(id_list=[arxiv_id])\\n        for result in client.results(search):\\n            out_path = output_dir / f\"{arxiv_id}.tar.gz\"\\n            result.download_source(str(output_dir), filename=f\"{arxiv_id}.tar.gz\")\\n            time.sleep(query_delay)\\n            return out_path\\n    except Exception as e:\\n        print(f\"Failed to download {arxiv_id}: {e}\")\\n        return None\\n\\n\\ndef extract_and_clean(file_path: Path, work_root: Path):\\n    #Gi·∫£i n√©n, l·ªçc gi·ªØ l·∫°i .tex/.bib, x√≥a r√°c & file n√©n\\n    try:\\n        # T·∫°o th∆∞ m·ª•c ƒë√≠ch: papers/<id_version>/\\n        id_version = file_path.name.replace(\".tar.gz\", \"\").replace(\".gz\", \"\").replace(\".tar\", \"\")\\n        target_dir = work_root / \"paper\" / id_version\\n        target_dir.mkdir(parents=True, exist_ok=True)\\n\\n        extract_until_done(file_path, target_dir)\\n        clean_and_flatten_subdirs(target_dir)\\n        delete_image_files(target_dir)\\n        delete_non_tex_files(target_dir)\\n        delete_empty_dirs(target_dir)\\n\\n        file_path.unlink(missing_ok=True)  # x√≥a file n√©n g·ªëc\\n        print(f\"{id_version}: Done extract + clean\")\\n\\n    except Exception as e:\\n        print(f\"{file_path.name}: {e}\")\\n\\n\\ndef worker_pipeline(queue, client, tmp_dir, work_root, query_delay):\\n    #Thread worker ch√≠nh: t·∫£i + gi·∫£i n√©n + d·ªçn d·∫πp\\n    while True:\\n        arxiv_id = queue.get()\\n        if arxiv_id is None:  # t√≠n hi·ªáu k·∫øt th√∫c\\n            break\\n        file_path = download_version(arxiv_id, client, tmp_dir, query_delay)\\n        if file_path and file_path.exists():\\n            extract_and_clean(file_path, work_root)\\n        queue.task_done()\\n\\n\\ndef run_concurrent_pipeline(version_list, work_root, max_workers=4):\\n    #Kh·ªüi ƒë·ªông pipeline ƒëa lu·ªìng cho to√†n b·ªô version\\n    tmp_dir = work_root / \"tmp_downloads\"\\n    tmp_dir.mkdir(parents=True, exist_ok=True)\\n\\n    client = arxiv.Client(page_size=1, delay_seconds=0.2, num_retries=3)\\n\\n    # T·∫°o h√†ng ƒë·ª£i v√† kh·ªüi t·∫°o thread pool\\n    queue = Queue()\\n    threads = []\\n    for _ in range(max_workers):\\n        t = Thread(target=worker_pipeline, args=(queue, client, tmp_dir, work_root, 1.5))\\n        t.start()\\n        threads.append(t)\\n\\n    # Th√™m t·∫•t c·∫£ version v√†o h√†ng ƒë·ª£i\\n    for vid in version_list:\\n        queue.put(vid)\\n\\n    queue.join()  # ƒë·ª£i to√†n b·ªô task xong\\n    for _ in range(max_workers):\\n        queue.put(None)\\n    for t in threads:\\n        t.join()\\n\\n    print(\"All papers processed successfully!\")\\n'"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import shutil, time\n",
    "\n",
    "def download_version(arxiv_id, client, output_dir, query_delay):\n",
    "    #T·∫£i 1 version paper t·ª´ arXiv v√† tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n file .tar.gz\n",
    "    try:\n",
    "        search = arxiv.Search(id_list=[arxiv_id])\n",
    "        for result in client.results(search):\n",
    "            out_path = output_dir / f\"{arxiv_id}.tar.gz\"\n",
    "            result.download_source(str(output_dir), filename=f\"{arxiv_id}.tar.gz\")\n",
    "            time.sleep(query_delay)\n",
    "            return out_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {arxiv_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_and_clean(file_path: Path, work_root: Path):\n",
    "    #Gi·∫£i n√©n, l·ªçc gi·ªØ l·∫°i .tex/.bib, x√≥a r√°c & file n√©n\n",
    "    try:\n",
    "        # T·∫°o th∆∞ m·ª•c ƒë√≠ch: papers/<id_version>/\n",
    "        id_version = file_path.name.replace(\".tar.gz\", \"\").replace(\".gz\", \"\").replace(\".tar\", \"\")\n",
    "        target_dir = work_root / \"paper\" / id_version\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        extract_until_done(file_path, target_dir)\n",
    "        clean_and_flatten_subdirs(target_dir)\n",
    "        delete_image_files(target_dir)\n",
    "        delete_non_tex_files(target_dir)\n",
    "        delete_empty_dirs(target_dir)\n",
    "\n",
    "        file_path.unlink(missing_ok=True)  # x√≥a file n√©n g·ªëc\n",
    "        print(f\"{id_version}: Done extract + clean\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{file_path.name}: {e}\")\n",
    "\n",
    "\n",
    "def worker_pipeline(queue, client, tmp_dir, work_root, query_delay):\n",
    "    #Thread worker ch√≠nh: t·∫£i + gi·∫£i n√©n + d·ªçn d·∫πp\n",
    "    while True:\n",
    "        arxiv_id = queue.get()\n",
    "        if arxiv_id is None:  # t√≠n hi·ªáu k·∫øt th√∫c\n",
    "            break\n",
    "        file_path = download_version(arxiv_id, client, tmp_dir, query_delay)\n",
    "        if file_path and file_path.exists():\n",
    "            extract_and_clean(file_path, work_root)\n",
    "        queue.task_done()\n",
    "\n",
    "\n",
    "def run_concurrent_pipeline(version_list, work_root, max_workers=4):\n",
    "    #Kh·ªüi ƒë·ªông pipeline ƒëa lu·ªìng cho to√†n b·ªô version\n",
    "    tmp_dir = work_root / \"tmp_downloads\"\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    client = arxiv.Client(page_size=1, delay_seconds=0.2, num_retries=3)\n",
    "\n",
    "    # T·∫°o h√†ng ƒë·ª£i v√† kh·ªüi t·∫°o thread pool\n",
    "    queue = Queue()\n",
    "    threads = []\n",
    "    for _ in range(max_workers):\n",
    "        t = Thread(target=worker_pipeline, args=(queue, client, tmp_dir, work_root, 1.5))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # Th√™m t·∫•t c·∫£ version v√†o h√†ng ƒë·ª£i\n",
    "    for vid in version_list:\n",
    "        queue.put(vid)\n",
    "\n",
    "    queue.join()  # ƒë·ª£i to√†n b·ªô task xong\n",
    "    for _ in range(max_workers):\n",
    "        queue.put(None)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    print(\"All papers processed successfully!\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dbde18",
   "metadata": {},
   "source": [
    "### **3 lu·ªìng 12 t·∫ßng song song**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "de1eee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import shutil, time, arxiv\n",
    "\n",
    "# ========= Helpers =========\n",
    "\n",
    "def strip_compression_suffix(filename: str) -> str:\n",
    "    # B·ªè m·ªçi ƒëu√¥i n√©n ph·ªï bi·∫øn: .tar.gz, .tgz, .gz, .tar, .xz, .zip\n",
    "    for ext in [\".tar.gz\", \".tgz\", \".tar.xz\", \".txz\", \".gz\", \".xz\", \".zip\", \".tar\"]:\n",
    "        if filename.endswith(ext):\n",
    "            return filename[: -len(ext)]\n",
    "    return filename\n",
    "\n",
    "# ========= Stage workers =========\n",
    "\n",
    "def downloader_worker(q_ids: Queue,\n",
    "                      q_to_extract: Queue,\n",
    "                      client: arxiv.Client,\n",
    "                      tmp_dir: Path,\n",
    "                      query_delay: float = 1.5):\n",
    "    \"\"\"\n",
    "    Nh·∫≠n arxiv_id_version (vd '2301.01234v2') -> t·∫£i .tar.gz v√†o tmp_dir\n",
    "    -> ƒë∆∞a ƒë∆∞·ªùng d·∫´n file n√©n sang h√†ng ƒë·ª£i extract.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        arxiv_id = q_ids.get()\n",
    "        if arxiv_id is None:\n",
    "            q_ids.task_done()\n",
    "            break\n",
    "        try:\n",
    "            search = arxiv.Search(id_list=[arxiv_id])\n",
    "            out_path = tmp_dir / f\"{arxiv_id}.tar.gz\"\n",
    "            for result in client.results(search):\n",
    "                # t·∫£i v·ªÅ file n√©n (n·∫øu ƒë√£ t·ªìn t·∫°i th√¨ ghi ƒë√® cho ch·∫Øc ch·∫Øn)\n",
    "                result.download_source(str(tmp_dir), filename=out_path.name)\n",
    "                time.sleep(query_delay)\n",
    "                break\n",
    "            if out_path.exists():\n",
    "                q_to_extract.put(out_path)\n",
    "                print(f\"[DL] {arxiv_id} -> {out_path.name}\")\n",
    "            else:\n",
    "                print(f\"[DL] {arxiv_id}: no archive written\")\n",
    "        except Exception as e:\n",
    "            print(f\"[DL] {arxiv_id}: {e}\")\n",
    "        finally:\n",
    "            q_ids.task_done()\n",
    "\n",
    "\n",
    "def extractor_worker(q_from_dl: Queue,\n",
    "                     q_to_clean: Queue,\n",
    "                     papers_root: Path):\n",
    "    \"\"\"\n",
    "    Nh·∫≠n file .tar.gz t·ª´ downloader -> gi·∫£i n√©n v√†o papers/<id_version>/\n",
    "    -> ƒë·∫©y th∆∞ m·ª•c ƒë√≠ch sang h√†ng ƒë·ª£i clean -> x√≥a file n√©n.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        archive_path = q_from_dl.get()\n",
    "        if archive_path is None:\n",
    "            q_from_dl.task_done()\n",
    "            break\n",
    "        try:\n",
    "            id_version = strip_compression_suffix(archive_path.name)\n",
    "            target_dir = papers_root / id_version\n",
    "            ensure_dir(target_dir)\n",
    "\n",
    "            print(f\"[EX] Extract {archive_path.name} ‚Üí {target_dir.name}\")\n",
    "            extract_until_done(archive_path, target_dir, max_depth=5)\n",
    "\n",
    "            # chuy·ªÉn sang stage d·ªçn d·∫πp\n",
    "            q_to_clean.put(target_dir)\n",
    "\n",
    "            # x√≥a file n√©n ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng\n",
    "            try:\n",
    "                archive_path.unlink(missing_ok=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[EX] cannot remove archive {archive_path}: {e}\")\n",
    "\n",
    "            print(f\"[EX] {id_version}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[EX] {archive_path.name}: {e}\")\n",
    "        finally:\n",
    "            q_from_dl.task_done()\n",
    "\n",
    "\n",
    "def cleaner_worker(q_from_extract: Queue):\n",
    "    \"\"\"\n",
    "    Nh·∫≠n th∆∞ m·ª•c papers/<id_version>/ -> di chuy·ªÉn .tex/.bib l√™n c·∫•p tr√™n n·∫øu c·∫ßn,\n",
    "    x√≥a ·∫£nh, x√≥a file r√°c, x√≥a th∆∞ m·ª•c r·ªóng.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        paper_dir = q_from_extract.get()\n",
    "        if paper_dir is None:\n",
    "            q_from_extract.task_done()\n",
    "            break\n",
    "        try:\n",
    "            # l√†m ph·∫≥ng .tex/.bib t·ª´ subdir, x√≥a subdir tr√πng/d∆∞\n",
    "            clean_and_flatten_subdirs(paper_dir)\n",
    "\n",
    "            # comment \\includegraphics ƒë·ªÉ tr√°nh l·ªói bi√™n d·ªãch\n",
    "            for tex_file in paper_dir.rglob(\"*.tex\"):\n",
    "                comment_include_graphics(tex_file)\n",
    "\n",
    "            # x√≥a ·∫£nh v√† c√°c file kh√¥ng ph·∫£i .tex/.bib\n",
    "            delete_image_files(paper_dir)\n",
    "            delete_non_tex_files(paper_dir)\n",
    "\n",
    "            # d·ªçn th∆∞ m·ª•c r·ªóng c√≤n s√≥t\n",
    "            delete_empty_dirs(paper_dir)\n",
    "\n",
    "            print(f\"[CL] cleaned {paper_dir.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[CL] {paper_dir}: {e}\")\n",
    "        finally:\n",
    "            q_from_extract.task_done()\n",
    "\n",
    "\n",
    "# ========= Orchestrator =========\n",
    "\n",
    "def run_three_stage_pipeline(version_list,\n",
    "                             work_root: Path,\n",
    "                             workers_per_stage: int = 4,\n",
    "                             q_maxsize: int = 64,\n",
    "                             client_delay: float = 0.2,\n",
    "                             client_retries: int = 3):\n",
    "    \"\"\"\n",
    "    Pipeline 3 t·∫ßng:\n",
    "      Stage 1 (Download):  workers_per_stage threads\n",
    "      Stage 2 (Extract):   workers_per_stage threads\n",
    "      Stage 3 (Clean):     workers_per_stage threads\n",
    "    H√†ng ƒë·ª£i c√≥ backpressure (q_maxsize) ƒë·ªÉ kh√¥ng ph√¨nh dung l∆∞·ª£ng t·∫°m.\n",
    "    \"\"\"\n",
    "\n",
    "    # Th∆∞ m·ª•c g·ªëc\n",
    "    tmp_dir    = work_root / \"tmp_downloads\"\n",
    "    papers_dir = work_root / \"paper\"\n",
    "    ensure_dir(tmp_dir)\n",
    "    ensure_dir(papers_dir)\n",
    "\n",
    "    # arxiv client d√πng chung (thread-safe ƒë·ªçc)\n",
    "    client = arxiv.Client(page_size=1, delay_seconds=client_delay, num_retries=client_retries)\n",
    "\n",
    "    # H√†ng ƒë·ª£i gi·ªØa c√°c stage (c√≥ gi·ªõi h·∫°n ƒë·ªÉ ch·ªëng ph√¨nh file t·∫°m)\n",
    "    q_ids        = Queue(maxsize=q_maxsize)  # input: version_list\n",
    "    q_to_extract = Queue(maxsize=q_maxsize)  # output c·ªßa DL -> input EX\n",
    "    q_to_clean   = Queue(maxsize=q_maxsize)  # output c·ªßa EX -> input CL\n",
    "\n",
    "    # Kh·ªüi t·∫°o worker c·ªßa t·ª´ng stage\n",
    "    dl_threads = [\n",
    "        Thread(target=downloader_worker, args=(q_ids, q_to_extract, client, tmp_dir), daemon=True)\n",
    "        for _ in range(workers_per_stage)\n",
    "    ]\n",
    "    ex_threads = [\n",
    "        Thread(target=extractor_worker, args=(q_to_extract, q_to_clean, papers_dir), daemon=True)\n",
    "        for _ in range(workers_per_stage)\n",
    "    ]\n",
    "    cl_threads = [\n",
    "        Thread(target=cleaner_worker, args=(q_to_clean,), daemon=True)\n",
    "        for _ in range(workers_per_stage)\n",
    "    ]\n",
    "\n",
    "    # Start t·∫•t c·∫£ threads\n",
    "    for t in dl_threads + ex_threads + cl_threads:\n",
    "        t.start()\n",
    "\n",
    "    # B∆°m to√†n b·ªô version v√†o h√†ng ƒë·ª£i ƒë·∫ßu v√†o\n",
    "    for vid in version_list:\n",
    "        q_ids.put(vid)\n",
    "\n",
    "    # Ch·ªù stage 1 ho√†n t·∫•t\n",
    "    q_ids.join()\n",
    "    # G·ª≠i t√≠n hi·ªáu d·ª´ng cho stage 1\n",
    "    for _ in dl_threads:\n",
    "        q_ids.put(None)\n",
    "    for t in dl_threads:\n",
    "        t.join()\n",
    "\n",
    "    # Ch·ªù stage 2 ho√†n t·∫•t\n",
    "    q_to_extract.join()\n",
    "    # G·ª≠i t√≠n hi·ªáu d·ª´ng cho stage 2\n",
    "    for _ in ex_threads:\n",
    "        q_to_extract.put(None)\n",
    "    for t in ex_threads:\n",
    "        t.join()\n",
    "\n",
    "    # Ch·ªù stage 3 ho√†n t·∫•t\n",
    "    q_to_clean.join()\n",
    "    # G·ª≠i t√≠n hi·ªáu d·ª´ng cho stage 3\n",
    "    for _ in cl_threads:\n",
    "        q_to_clean.put(None)\n",
    "    for t in cl_threads:\n",
    "        t.join()\n",
    "\n",
    "    print(\"Three-stage pipeline finished successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1d17c",
   "metadata": {},
   "source": [
    "### **Ch·∫°y ch∆∞∆°ng tr√¨nh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "48b03ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating metadata for 2211.13747...\n",
      "Creating metadata for 2211.13748...\n",
      "Creating metadata for 2211.13749...\n",
      "Creating metadata for 2211.13750...\n",
      "Fetched metadata for arXiv ID 2211.13747\n",
      "Creating metadata for 2211.13751...\n",
      "Fetched metadata for arXiv ID 2211.13748Fetched metadata for arXiv ID 2211.13750\n",
      "\n",
      "Creating metadata for 2211.13752...\n",
      "Creating metadata for 2211.13753...\n",
      "Fetched metadata for arXiv ID 2211.13749\n",
      "Creating metadata for 2211.13754...\n",
      "Fetched metadata for arXiv ID 2211.13751\n",
      "Creating metadata for 2211.13755...\n",
      "Fetched metadata for arXiv ID 2211.13752\n",
      "Fetched metadata for arXiv ID 2211.13753\n",
      "Fetched metadata for arXiv ID 2211.13754\n",
      "Fetched metadata for arXiv ID 2211.13755\n",
      "Saved checkpoint 1.\n",
      "Total versions to download: 11\n",
      "Total versions to download: 11\n",
      "[DL] 2211.13749v1 -> 2211.13749v1.tar.gz\n",
      "[EX] Extract 2211.13749v1.tar.gz ‚Üí 2211.13749v1\n",
      "[0] Extracting GZIP file: workdir\\tmp_downloads\\2211.13749v1.tar.gz\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "D·ªçn 2211.13749v1: di chuy·ªÉn 0 file, x√≥a 0 th∆∞ m·ª•c con.\n",
      "[EX] 2211.13749v1\n",
      "D·ªçn d·∫πp '2211.13749v1': gi·ªØ 1, x√≥a 0 file kh√°c.\n",
      "[CL] cleaned 2211.13749v1\n",
      "[DL] 2211.13750v1 -> 2211.13750v1.tar.gz\n",
      "[EX] Extract 2211.13750v1.tar.gz ‚Üí 2211.13750v1\n",
      "[0] Extracting TAR archive: workdir\\tmp_downloads\\2211.13750v1.tar.gz\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "[EX] 2211.13750v1\n",
      "D·ªçn 2211.13750v1: di chuy·ªÉn 0 file, x√≥a 0 th∆∞ m·ª•c con.\n",
      "Commented 3 includegraphics in comparingsinglettestingschemes.tex\n",
      "Removed 3 image files from 2211.13750v1\n",
      "D·ªçn d·∫πp '2211.13750v1': gi·ªØ 2, x√≥a 1 file kh√°c.\n",
      "[CL] cleaned 2211.13750v1\n",
      "[DL] 2211.13748v1 -> 2211.13748v1.tar.gz\n",
      "[EX] Extract 2211.13748v1.tar.gz ‚Üí 2211.13748v1\n",
      "[0] Extracting TAR archive: workdir\\tmp_downloads\\2211.13748v1.tar.gz\n",
      "[DL] 2211.13747v1 -> 2211.13747v1.tar.gz\n",
      "[EX] Extract 2211.13747v1.tar.gz ‚Üí 2211.13747v1\n",
      "[0] Extracting GZIP file: workdir\\tmp_downloads\\2211.13747v1.tar.gz\n",
      "Kh√¥ng th·ªÉ gi·∫£i n√©n 2211.13747v1.tar.gz: Not a gzipped file (b'%P')\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "[EX] 2211.13747v1\n",
      "D·ªçn 2211.13747v1: di chuy·ªÉn 0 file, x√≥a 0 th∆∞ m·ª•c con.\n",
      "D·ªçn d·∫πp '2211.13747v1': gi·ªØ 0, x√≥a 1 file kh√°c.\n",
      "[CL] cleaned 2211.13747v1\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "[EX] 2211.13748v1\n",
      "D·ªçn 2211.13748v1: di chuy·ªÉn 0 file, x√≥a 2 th∆∞ m·ª•c con.\n",
      "Commented 4 includegraphics in weibo.tex\n",
      "Removed 2 image files from 2211.13748v1\n",
      "D·ªçn d·∫πp '2211.13748v1': gi·ªØ 2, x√≥a 6 file kh√°c.\n",
      "[CL] cleaned 2211.13748v1\n",
      "[DL] 2211.13750v2 -> 2211.13750v2.tar.gz\n",
      "[EX] Extract 2211.13750v2.tar.gz ‚Üí 2211.13750v2\n",
      "[0] Extracting TAR archive: workdir\\tmp_downloads\\2211.13750v2.tar.gz\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "D·ªçn 2211.13750v2: di chuy·ªÉn 0 file, x√≥a 0 th∆∞ m·ª•c con.\n",
      "[EX] 2211.13750v2\n",
      "Commented 3 includegraphics in comparingsinglettestingschemes4.tex\n",
      "Removed 3 image files from 2211.13750v2\n",
      "D·ªçn d·∫πp '2211.13750v2': gi·ªØ 2, x√≥a 1 file kh√°c.\n",
      "[CL] cleaned 2211.13750v2\n",
      "[DL] 2211.13751v1 -> 2211.13751v1.tar.gz\n",
      "[EX] Extract 2211.13751v1.tar.gz ‚Üí 2211.13751v1\n",
      "[0] Extracting TAR archive: workdir\\tmp_downloads\\2211.13751v1.tar.gz\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "[EX] 2211.13751v1\n",
      "D·ªçn 2211.13751v1: di chuy·ªÉn 0 file, x√≥a 1 th∆∞ m·ª•c con.\n",
      "Commented 9 includegraphics in Kane_et_al_JFM_v1.tex\n",
      "D·ªçn d·∫πp '2211.13751v1': gi·ªØ 2, x√≥a 5 file kh√°c.\n",
      "[CL] cleaned 2211.13751v1\n",
      "[DL] 2211.13754v1 -> 2211.13754v1.tar.gz\n",
      "[EX] Extract 2211.13754v1.tar.gz ‚Üí 2211.13754v1\n",
      "[0] Extracting TAR archive: workdir\\tmp_downloads\\2211.13754v1.tar.gz\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "[EX] 2211.13754v1\n",
      "D·ªçn 2211.13754v1: di chuy·ªÉn 0 file, x√≥a 0 th∆∞ m·ª•c con.\n",
      "Commented 11 includegraphics in main.tex\n",
      "Removed 10 image files from 2211.13754v1\n",
      "D·ªçn d·∫πp '2211.13754v1': gi·ªØ 4, x√≥a 1 file kh√°c.\n",
      "[CL] cleaned 2211.13754v1\n",
      "[DL] 2211.13753v1 -> 2211.13753v1.tar.gz\n",
      "[EX] Extract 2211.13753v1.tar.gz ‚Üí 2211.13753v1\n",
      "[0] Extracting TAR archive: workdir\\tmp_downloads\\2211.13753v1.tar.gz\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "[EX] 2211.13753v1\n",
      "D·ªçn 2211.13753v1: di chuy·ªÉn 0 file, x√≥a 1 th∆∞ m·ª•c con.\n",
      "Commented 8 includegraphics in 6-toymodel.tex\n",
      "D·ªçn d·∫πp '2211.13753v1': gi·ªØ 12, x√≥a 3 file kh√°c.\n",
      "[CL] cleaned 2211.13753v1\n",
      "[DL] 2211.13752v1 -> 2211.13752v1.tar.gz\n",
      "[EX] Extract 2211.13752v1.tar.gz ‚Üí 2211.13752v1\n",
      "[0] Extracting TAR archive: workdir\\tmp_downloads\\2211.13752v1.tar.gz\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "[EX] 2211.13752v1\n",
      "D·ªçn 2211.13752v1: di chuy·ªÉn 0 file, x√≥a 1 th∆∞ m·ª•c con.\n",
      "Commented 2 includegraphics in 03_method.tex\n",
      "Commented 11 includegraphics in 04_results.tex\n",
      "Commented 1 includegraphics in 05_conclusion.tex\n",
      "Commented 9 includegraphics in 06_appendix.tex\n",
      "Commented 1 includegraphics in main.tex\n",
      "D·ªçn d·∫πp '2211.13752v1': gi·ªØ 11, x√≥a 5 file kh√°c.\n",
      "[CL] cleaned 2211.13752v1\n",
      "[DL] 2211.13755v1 -> 2211.13755v1.tar.gz\n",
      "[EX] Extract 2211.13755v1.tar.gz ‚Üí 2211.13755v1\n",
      "[0] Extracting TAR archive: workdir\\tmp_downloads\\2211.13755v1.tar.gz\n",
      "[DL] 2211.13755v2 -> 2211.13755v2.tar.gz\n",
      "[EX] Extract 2211.13755v2.tar.gz ‚Üí 2211.13755v2\n",
      "[0] Extracting TAR archive: workdir\\tmp_downloads\\2211.13755v2.tar.gz\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "[EX] 2211.13755v1\n",
      "D·ªçn 2211.13755v1: di chuy·ªÉn 0 file, x√≥a 1 th∆∞ m·ª•c con.\n",
      "Commented 41 includegraphics in main.tex\n",
      "D·ªçn d·∫πp '2211.13755v1': gi·ªØ 1, x√≥a 3 file kh√°c.\n",
      "[CL] cleaned 2211.13755v1\n",
      "Ho√†n t·∫•t: kh√¥ng c√≤n file n√©n n√†o ƒë·ªÉ gi·∫£i.\n",
      "[EX] 2211.13755v2\n",
      "D·ªçn 2211.13755v2: di chuy·ªÉn 0 file, x√≥a 1 th∆∞ m·ª•c con.\n",
      "Commented 41 includegraphics in main.tex\n",
      "D·ªçn d·∫πp '2211.13755v2': gi·ªØ 2, x√≥a 3 file kh√°c.\n",
      "[CL] cleaned 2211.13755v2\n",
      "Three-stage pipeline finished successfully!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = metadatas_dir\n",
    "\n",
    "crawl_arXiv_metadata(START_MONTH, START_ID, END_MONTH, END_ID,\n",
    "                     folder=checkpoint_dir)\n",
    "\n",
    "version_list = get_version_list_from_metadata(metadatas_dir)\n",
    "print(f\"Total versions to download: {len(version_list)}\")\n",
    "\n",
    "#run_concurrent_pipeline(\n",
    "    #version_list=version_list,\n",
    "    #work_root=WORK_DIR,\n",
    "    #max_workers=4\n",
    "#)\n",
    "\n",
    "run_three_stage_pipeline(\n",
    "    version_list=version_list,          # danh s√°ch nh∆∞ \"2301.01234v2\"\n",
    "    work_root=WORK_DIR,                 # th∆∞ m·ª•c l√†m vi·ªác g·ªëc c·ªßa b·∫°n\n",
    "    workers_per_stage=4,                # 4 lu·ªìng/t·∫ßng nh∆∞ b·∫°n y√™u c·∫ßu\n",
    "    q_maxsize=64,                       # backpressure ch·ªëng ph√¨nh file t·∫°m\n",
    "    client_delay=0.2,                   # arxiv.Client delay\n",
    "    client_retries=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
